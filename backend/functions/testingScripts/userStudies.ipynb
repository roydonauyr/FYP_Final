{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Of Vector Store Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin Yee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_SinYee = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_SinYee.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\faiss_vectorstore_open_ai_SinYee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "persona = \"\"\"You would be assisting in identifying topics from a snippet of conversation\"\"\"\n",
    "task = \"\"\"I would supply the conversation directly. Interpret the main topic of the conversation and return the main topic. Do not give multiple topics such as football/soccer. Only give one main topic.\"\"\"\n",
    "example = \"\"\"For example if the conversation is: \n",
    "            {\"Sin Yee\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Sin Yee! Yeah, it's always exciting to see how your team will perform.\"}\n",
    "\n",
    "           football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Sin Yee\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Sin Yee\": \"Hey there! How are you doing?\", \"John\": \"Hey Sin Yee! I'm doing great, how about you?\"}\n",
    "\n",
    "            general\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    instruction = f\"{persona} {task} {example}\"\n",
    "    messages = [SystemMessage(content=instruction)]\n",
    "\n",
    "    query = doc.page_content\n",
    "\n",
    "    usermsg = HumanMessage(content=query)\n",
    "    messages.append(usermsg)\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    doc.metadata['topic'] = response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"Xavier\": \"Me too, Sin Yee! Let's continue to inspire and motivate each other. I can't wait to see how far we'll both go in our crocheting journey.\"}' metadata={'label': 'Response 11', 'source': 'Xavier.json', 'topic': 'crocheting'}\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crochet': 1, 'art': 1, 'crafts': 1, 'crafting': 1, 'design': 1, 'gardening': 1, 'travel': 1, 'food': 1, 'fitness': 1, 'work': 1, 'healthcare': 1, 'health': 1, 'workload': 1, 'relaxation': 1, 'stress': 1, 'career': 1, 'nursing': 1, 'general': 1, 'crocheting': 1, 'knitting': 1, 'goal': 1, 'hobbies': 1, 'friendship': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x245d28a8b50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_SinYee = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_SinYee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_SinYee.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\faiss_vectorstore_SinYee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lee Hang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Lee Hang'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Lee Hang\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_Lee_Hang = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_Lee_Hang.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Lee Hang\\\\faiss_vectorstore_open_ai_Lee_Hang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You would be assisting in identifying topics from a snippet of conversation. I would supply the conversation directly. \n",
    "Interpret the main topic of the conversation and return the main topic.\n",
    "\n",
    "Do not give multiple topics such as football/soccer. Only give one main topic.\n",
    "\n",
    "For example if the conversation is: \n",
    "            {\"Lee Hang\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Lee Hang! Yeah, it's \n",
    "            always exciting to see how your team will perform.\"}\n",
    "\n",
    "            football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Lee Hang\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Lee Hang\": \"Hey there! How are you doing?\", \"John\": \"Hey Lee Hang! I'm doing great, how about you?\"}\n",
    "\n",
    "            general \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(meta_content, query):\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": meta_content,\n",
    "    }\n",
    "\n",
    "    #print(\"Query is: \" + query)\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    topic = raw_response.choices[0].message.content\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    \n",
    "    query = doc.page_content\n",
    "\n",
    "    doc.metadata['topic'] = getTopic(meta_content, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Lee Hang\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking techniques\n",
      "culinary arts\n",
      "cooking competition\n",
      "cooking show\n",
      "elderly care\n",
      "pet training\n",
      "pet training\n",
      "pet relations\n",
      "[6, 25, 27, 29, 55, 60, 61, 66]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[66].metadata['topic'] = \"pet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cooking': 1, 'television': 1, 'travel': 1, 'language': 1, 'technology': 1, 'fintech': 1, 'hackathon': 1, 'business': 1, 'volunteering': 1, 'activities': 1, 'stories': 1, 'elderly': 1, 'pets': 1, 'pet': 1, 'pet_adjustment': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x16bf0d26bd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_LeeHang = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_LeeHang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_LeeHang.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Lee Hang\\\\faiss_vectorstore_LeeHang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gregory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Gregory'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Gregory\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_Gregory = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_Gregory.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Gregory\\\\faiss_vectorstore_open_ai_Gregory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You would be assisting in identifying topics from a snippet of conversation. I would supply the conversation directly. \n",
    "Interpret the main topic of the conversation and return the main topic.\n",
    "\n",
    "Do not give multiple topics such as football/soccer. Only give one main topic.\n",
    "\n",
    "For example if the conversation is: \n",
    "            {\"Gregory\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Gregory! Yeah, it's \n",
    "            always exciting to see how your team will perform.\"}\n",
    "\n",
    "            football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Gregory\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Gregory\": \"Hey there! How are you doing?\", \"John\": \"Hey Gregory! I'm doing great, how about you?\"}\n",
    "\n",
    "            general \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(meta_content, query):\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": meta_content,\n",
    "    }\n",
    "\n",
    "    #print(\"Query is: \" + query)\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    topic = raw_response.choices[0].message.content\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    \n",
    "    query = doc.page_content\n",
    "\n",
    "    doc.metadata['topic'] = getTopic(meta_content, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Gregory\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video games\n",
      "puzzle games\n",
      "video games\n",
      "television drama\n",
      "[26, 27, 28, 32]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'travel': 1, 'food': 1, 'gaming': 1, 'video games': 1, 'puzzle games': 1, 'kdramas': 1, 'movies': 1, 'television drama': 1, 'television': 1, 'entertainment': 1, 'football': 1, 'sports': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1afff8de0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_Gregory = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_Gregory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_Gregory.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Gregory\\\\faiss_vectorstore_Gregory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheryl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Cheryl'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Cheryl\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_Cheryl = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_Cheryl.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Cheryl\\\\faiss_vectorstore_open_ai_Cheryl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You would be assisting in identifying topics from a snippet of conversation. I would supply the conversation directly. \n",
    "Interpret the main topic of the conversation and return the main topic.\n",
    "\n",
    "Do not give multiple topics such as football/soccer. Only give one main topic.\n",
    "\n",
    "For example if the conversation is: \n",
    "            {\"Cheryl\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Cheryl! Yeah, it's \n",
    "            always exciting to see how your team will perform.\"}\n",
    "\n",
    "            football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Cheryl\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Cheryl\": \"Hey there! How are you doing?\", \"John\": \"Hey Cheryl! I'm doing great, how about you?\"}\n",
    "\n",
    "            general \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(meta_content, query):\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": meta_content,\n",
    "    }\n",
    "\n",
    "    #print(\"Query is: \" + query)\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    topic = raw_response.choices[0].message.content\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    \n",
    "    query = doc.page_content\n",
    "\n",
    "    doc.metadata['topic'] = getTopic(meta_content, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Cheryl\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdoor activities\n",
      "Formula 1\n",
      "event planning\n",
      "marathon training\n",
      "[17, 18, 19, 46]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cooking': 1, 'decorating': 1, 'gifts': 1, 'travel': 1, 'hiking': 1, 'outdoor activities': 1, 'Formula 1': 1, 'event planning': 1, 'sports': 1, 'shopping': 1, 'racing': 1, 'promotions': 1, 'entertainment': 1, 'memories': 1, 'advice': 1, 'photography': 1, 'marathon': 1, 'fitness': 1, 'marathon training': 1, 'nutrition': 1, 'running': 1, 'exercise': 1, 'badminton': 1, 'sports ': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1afe69fe150>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_Cheryl = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_Cheryl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_Cheryl.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Cheryl\\\\faiss_vectorstore_Cheryl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yu Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Yu Min'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Yu Min\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_Yumin = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_Yumin.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Yu Min\\\\faiss_vectorstore_open_ai_Yumin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You would be assisting in identifying topics from a snippet of conversation. I would supply the conversation directly. \n",
    "Interpret the main topic of the conversation and return the main topic.\n",
    "\n",
    "Do not give multiple topics such as football/soccer. Only give one main topic.\n",
    "\n",
    "For example if the conversation is: \n",
    "            {\"Yu Min\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Yu Min! Yeah, it's \n",
    "            always exciting to see how your team will perform.\"}\n",
    "\n",
    "            football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Yu Min\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Yu Min\": \"Hey there! How are you doing?\", \"John\": \"Hey Yu Min! I'm doing great, how about you?\"}\n",
    "\n",
    "            general \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(meta_content, query):\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": meta_content,\n",
    "    }\n",
    "\n",
    "    #print(\"Query is: \" + query)\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    topic = raw_response.choices[0].message.content\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    \n",
    "    query = doc.page_content\n",
    "\n",
    "    doc.metadata['topic'] = getTopic(meta_content, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Yu Min\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adrenaline sports\n",
      "dining out\n",
      "event planning\n",
      "event planning\n",
      "event planning\n",
      "event planning\n",
      "video games\n",
      "[11, 20, 22, 27, 31, 35, 67]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[20].metadata['topic'] = \"event planning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adventure': 1, 'travel': 1, 'challenge': 1, 'skydiving': 1, 'cityscape': 1, 'party': 1, 'beverages': 1, 'event planning': 1, 'music': 1, 'events': 1, 'cooking': 1, 'baking': 1, 'hobbies': 1, 'relationship': 1, 'swimming': 1, 'education': 1, 'motivation': 1, 'sports': 1, 'video games': 1, 'floorball': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1afff8ea590>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_Yumin = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_Yumin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_Yumin.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Yu Min\\\\faiss_vectorstore_Yumin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final - Jane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Jane'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Jane\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_open_ai_Jane = FAISS.from_documents(documents, open_ai_embeddings)\n",
    "## Saving Vector Store\n",
    "faiss_vectorstore_open_ai_Jane.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Jane\\\\faiss_vectorstore_open_ai_Jane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You would be assisting in identifying topics from a snippet of conversation. I would supply the conversation directly. \n",
    "Interpret the main topic of the conversation and return the main topic.\n",
    "\n",
    "Do not give multiple topics such as football/soccer. Only give one main topic.\n",
    "\n",
    "For example if the conversation is: \n",
    "            {\"Jane\": \"Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Jane! Yeah, it's \n",
    "            always exciting to see how your team will perform.\"}\n",
    "\n",
    "            football\n",
    "\n",
    "            Example 2:\n",
    "            {\"Jane\": \"I'm planning to go on a trip to Japan next year\", \"John\": \"That's awesome! Japan is such a beautiful country.\"}\n",
    "\n",
    "            travel\n",
    "\n",
    "            Example 3: If no main topic can be determined such as a greeting\n",
    "            {\"Jane\": \"Hey there! How are you doing?\", \"John\": \"Hey Jane! I'm doing great, how about you?\"}\n",
    "\n",
    "            general \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(meta_content, query):\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": meta_content,\n",
    "    }\n",
    "\n",
    "    #print(\"Query is: \" + query)\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    topic = raw_response.choices[0].message.content\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    \n",
    "    query = doc.page_content\n",
    "\n",
    "    doc.metadata['topic'] = getTopic(meta_content, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the documents list to a JSON serializable format\n",
    "documents_json = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Specify the file path for the JSON file\n",
    "json_file_path = 'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Jane\\\\documents.json'\n",
    "\n",
    "# Save the documents list into a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(documents_json, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art or craft\n",
      "work or positivity\n",
      "[1, 42]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic'].split()) > 1):\n",
    "        print(doc.metadata['topic'])\n",
    "        position.append(count)\n",
    "    count += 1\n",
    "\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[42].metadata['topic'] = \"profession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if(len(doc.metadata['topic']) == 0):\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crafts': 1, 'crotchet': 1, 'crocheting': 1, 'crafting': 1, 'design': 1, 'arts': 1, 'hobby': 1, 'gardening': 1, 'travel': 1, 'surfing': 1, 'fitness': 1, 'healthcare': 1, 'health': 1, 'workload': 1, 'relaxation': 1, 'stress': 1, 'profession': 1, 'nursing': 1, 'inspiration': 1, 'knitting': 1, 'goal': 1, 'friendship': 1, 'hobbies': 1}\n"
     ]
    }
   ],
   "source": [
    "temp = {}\n",
    "\n",
    "for doc in documents:\n",
    "    if(doc.metadata['topic'] not in temp):\n",
    "        temp[doc.metadata['topic']] = 1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x23c7f910950>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector store\n",
    "faiss_vectorstore_Jane = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_Jane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_Jane.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Jane\\\\faiss_vectorstore_Jane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Improvement - Ranking Of Results and Regenerating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Hi Gary, I've been keeping busy with work and some new hobbies.\", \n",
    "    \"Hey Gary, I've been learning how to cook, how about you?\", \n",
    "    \"Hey Gary, I've been learning gardening, how about you?\"\n",
    "]\n",
    "\n",
    "query = \"Hey Jane, what have you been up to?\"\n",
    "\n",
    "scores = [0.5, 0, 1]\n",
    "\n",
    "contexts = \"\"\"{{\"Natty\": \"Hey Sin Yee, how's work at the hospital been lately?\", \"Sin Yee\": \"Hi Natty, work has been really tiring but fulfilling.\"}}{{\"Jackson\": \"What have you been up to Sin Yee!\", \"Sin Yee\": \"Hey Jackson, guess what? I've picked up gardening as a new hobby!\"}}{{\"Mary\": \"Surfing sounds like a fun challenge! I'll make sure to pack my swimsuit. Do you have any favorite restaurants in Sydney?\", \"Sin Yee\": \"There's this amazing seafood restaurant by the harbor that we have to try. The views are incredible!\"}}\"\"\"\n",
    "\n",
    "badResponse = responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = f\"\"\"\n",
    "You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Jane and the normal person is indicated as Gary.\n",
    "You previously generated the following responses:\n",
    "{responses}\n",
    "For the query of: {query}\n",
    "\n",
    "The score of the responses are as follows with a max score of 1 being a good response, 0.5 being neutral and below 0.5 being a bad response:\n",
    "{scores}\n",
    "\n",
    "You have to generate a response that would replace the {badResponse} using the other good responses (above 0.5) as a guide.\n",
    "The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "\n",
    "Snippets of conversation would be given below in the section of Context. Use the conversations to assist in the generation the responses. Primarily the topic should be inferred from the question asked but if no topic can be inferred, infer the topics from the conversations given in the context. The conversations are seperated by \"{{\" and \"}}\":\\n\n",
    "Context: {contexts}\n",
    "\n",
    "For example, if the context above contains \"{{\"Roydon\": \"Recently my new pet dog has been so fun!\", \"Jacob\": \"That\\'s awesome! What breed is it?\"}}\"\n",
    "\n",
    "If the user asks \"What have you been up to?\"\n",
    "\n",
    "An example of the generated response would be in the format of 1 single string \"I've been taking care of my new pet dog, it's been so fun!\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been learning gardening, how about you?\n"
     ]
    }
   ],
   "source": [
    "# Learning instructions\n",
    "instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": meta_content,\n",
    "}\n",
    "\n",
    "#print(\"Query is: \" + query)\n",
    "\n",
    "# Initialize messages\n",
    "messages = []\n",
    "\n",
    "# Add learn instruction to message array\n",
    "messages.append(instruction)\n",
    "\n",
    "user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "}\n",
    "\n",
    "messages.append(user_message)\n",
    "\n",
    "openai.api_type = 'openai'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    ")\n",
    "newResponse = raw_response.choices[0].message.content\n",
    "\n",
    "print(newResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = \"\"\"\n",
    "You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Jane and the normal person is indicated as Gary.\n",
    "You previously generated the following responses:\n",
    "{responses}\n",
    "For the query of: {query}\n",
    "\n",
    "The score of the responses are as follows respectively with a max score of 1 being a good response, 0.5 being neutral and below 0.5 being a bad response:\n",
    "{scores}\n",
    "\n",
    "You have to generate a response that would replace the {badResponse} using the other responses (above 0.5) as a guide. Use responses with higher scores first (those with score of 1).\n",
    "The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "\n",
    "Snippets of conversation would be given below in the section of Context. Use the conversations to assist in the generation the responses. Primarily the topic should be inferred from the question asked but if no topic can be inferred, infer the topics from the conversations given in the context. The conversations are seperated by \"{{\" and \"}}\":\\n\n",
    "Context: {contexts}\n",
    "\n",
    "For example, if the responses previously generated were:\n",
    "[\"Hi Gary, I've been keeping busy with work and some new hobbies.\", \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "and the scores are [0.5, 0, 1] respectively,\n",
    "then, the good response is: \"Hey Gary, I've been learning gardening, how about you\"?\n",
    "and the bad response is \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\"\n",
    "\n",
    "The generated response should not be about cooking and should be about gardening (using the good response as a guide to infer the topic): \"I've been learning gardening, it's been so fun!\". But is should not repeat existing responses like I've been busy with work and some new hobbies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Hi Gary, I've been keeping busy with work and some new hobbies.\", \n",
    "    \"Hey Gary, I've been learning how to hike, how about you?\", \n",
    "    \"Hey Gary, I've been learning gardening, how about you?\"\n",
    "]\n",
    "\n",
    "query = \"Hey Jane, what are you up to?\"\n",
    "\n",
    "scores = [0.5, 0, 1]\n",
    "\n",
    "contexts = \"\"\"{{\"Natty\": \"Hey Sin Yee, how's work at the hospital been lately?\", \"Sin Yee\": \"Hi Natty, work has been really tiring but fulfilling.\"}}{{\"Jackson\": \"What have you been up to Sin Yee!\", \"Sin Yee\": \"Hey Jackson, guess what? I've picked up gardening as a new hobby!\"}}{{\"Mary\": \"Surfing sounds like a fun challenge! I'll make sure to pack my swimsuit. Do you have any favorite restaurants in Sydney?\", \"Sin Yee\": \"There's this amazing seafood restaurant by the harbor that we have to try. The views are incredible!\"}}\"\"\"\n",
    "\n",
    "badResponse = responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = f\"\"\"\n",
    "You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Jane and the normal person is indicated as Gary.\n",
    "You previously generated the following responses:\n",
    "{responses}\n",
    "For the query of: {query}\n",
    "\n",
    "The score of the responses are as follows respectively with a max score of 1 being a good response, 0.5 being neutral and below 0.5 being a bad response:\n",
    "{scores}\n",
    "\n",
    "You have to generate a response that would replace the {badResponse} using the other responses (above 0.5) as a guide. Use responses with higher scores first (those with score of 1).\n",
    "The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "\n",
    "For example, \n",
    "If the query is: \"Hey Jane, what are you up to?\"\n",
    "and the responses previously generated were:\n",
    "[\"Hi Gary, I've been keeping busy with work and some new hobbies.\", \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "and the scores are [0.5, 0, 1] respectively,\n",
    "then, the good response is: \"Hey Gary, I've been learning gardening, how about you\"?\n",
    "and the bad response is \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\"\n",
    "\n",
    "The generated response should not be about cooking and should be about gardening (using the good response as a guide to infer the topic): \n",
    "Response: \"I've been learning gardening, it's been so fun!\". But it should not repeat existing responses like I've been busy with work and some new hobbies.\n",
    "\n",
    "Another example:\n",
    "If the query is: \"Hey Jane, what are you up to?\"\n",
    "and the responses previously generated were:\n",
    "[\"Hi Gary, I've been dancing recently.\", \"Hey Gary, I've been hiking recently!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "and the scores are [1, 0, 0.5] respectively,\n",
    "then, the good response is: \"Hi Gary, I've been dancing recently.\"?\n",
    "and the bad response is \"Hey Gary, I've been hiking recently!\"\n",
    "\n",
    "The generated response should not be about hiking and should be about dancing (using the good response as a guide to infer the topic): \n",
    "\n",
    "Response: \"I've been learning how to dance and its my new hobby\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been learning how to dance and it's my new hobby.\n"
     ]
    }
   ],
   "source": [
    "# Learning instructions\n",
    "instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": meta_content,\n",
    "}\n",
    "\n",
    "#print(\"Query is: \" + query)\n",
    "\n",
    "# Initialize messages\n",
    "messages = []\n",
    "\n",
    "# Add learn instruction to message array\n",
    "messages.append(instruction)\n",
    "\n",
    "user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "}\n",
    "\n",
    "messages.append(user_message)\n",
    "\n",
    "openai.api_type = 'openai'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    ")\n",
    "newResponse = raw_response.choices[0].message.content\n",
    "\n",
    "print(newResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Hi Gary, I've been keeping busy with work and some new hobbies.\", \n",
    "    \"Hey Gary, I've been learning how to dance, how about you?\", \n",
    "    \"Hey Gary, I've been learning gardening, how about you?\"\n",
    "]\n",
    "\n",
    "query = \"Hey Jane, what are you up to?\"\n",
    "\n",
    "scores = [0.5, 0, 1]\n",
    "\n",
    "contexts = \"\"\"{{\"Natty\": \"Hey Sin Yee, how's work at the hospital been lately?\", \"Sin Yee\": \"Hi Natty, work has been really tiring but fulfilling.\"}}{{\"Jackson\": \"What have you been up to Sin Yee!\", \"Sin Yee\": \"Hey Jackson, guess what? I've picked up gardening as a new hobby!\"}}{{\"Mary\": \"Surfing sounds like a fun challenge! I'll make sure to pack my swimsuit. Do you have any favorite restaurants in Sydney?\", \"Sin Yee\": \"There's this amazing seafood restaurant by the harbor that we have to try. The views are incredible!\"}}\"\"\"\n",
    "\n",
    "badResponse = responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = f\"\"\"\n",
    "You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Jane and the normal person is indicated as Gary.\n",
    "You previously generated the following responses:\n",
    "{responses}\n",
    "For the query of: {query}\n",
    "\n",
    "The score of the responses are as follows respectively with a max score of 1 being a good response, 0.5 being neutral and below 0.5 being a bad response:\n",
    "{scores}\n",
    "\n",
    "You have to generate a response that would replace the {badResponse} using the other responses (above 0.5) as a guide. Use responses with higher scores first (those with score of 1).\n",
    "The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "\n",
    "For example, \n",
    "If the query is: \"Hey Jane, what are you up to?\"\n",
    "and the responses previously generated were:\n",
    "[\"Hi Gary, I've been keeping busy with work and some new hobbies.\", \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "and the scores are [0.5, 0, 1] respectively,\n",
    "then, the good response is: \"Hey Gary, I've been learning gardening, how about you\"?\n",
    "and the bad response is \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\"\n",
    "\n",
    "The generated response should not be about cooking and should be about gardening (using the good response as a guide to infer the topic): \n",
    "Response: \"I've been learning gardening, it's been so fun!\". But is should not repeat existing responses like I've been busy with work and some new hobbies.\n",
    "\n",
    "Another example:\n",
    "If the query is: \"Hey Jane, what are you up to?\"\n",
    "and the responses previously generated were:\n",
    "[\"Hi Gary, I've been dancing recently.\", \"Hey Gary, I've been hiking recently!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "and the scores are [1, 0, 0.5] respectively,\n",
    "then, the good response is: \"Hi Gary, I've been dancing recently.\"?\n",
    "and the bad response is \"Hey Gary, I've been hiking recently!\"\n",
    "\n",
    "The generated response should not be about hiking and should be about dancing (using the good response as a guide to infer the topic): \n",
    "\n",
    "Response: \"I've been learning how to dance and its my new hobby\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been learning gardening and it's been so rewarding!\n"
     ]
    }
   ],
   "source": [
    "# Learning instructions\n",
    "instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": meta_content,\n",
    "}\n",
    "\n",
    "#print(\"Query is: \" + query)\n",
    "\n",
    "# Initialize messages\n",
    "messages = []\n",
    "\n",
    "# Add learn instruction to message array\n",
    "messages.append(instruction)\n",
    "\n",
    "user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "}\n",
    "\n",
    "messages.append(user_message)\n",
    "\n",
    "openai.api_type = 'openai'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    ")\n",
    "newResponse = raw_response.choices[0].message.content\n",
    "\n",
    "print(newResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Hi Gary, I've been keeping busy with work and some new hobbies.\", \n",
    "    \"Hey Gary, I've been learning how to dance, how about you?\", \n",
    "    \"Hey Gary, I've been learning gardening, how about you?\"\n",
    "]\n",
    "\n",
    "query = \"Hey Jane, what are you up to?\"\n",
    "\n",
    "scores = [0.5, 1, 0]\n",
    "\n",
    "contexts = \"\"\"{{\"Natty\": \"Hey Sin Yee, how's work at the hospital been lately?\", \"Sin Yee\": \"Hi Natty, work has been really tiring but fulfilling.\"}}{{\"Jackson\": \"What have you been up to Sin Yee!\", \"Sin Yee\": \"Hey Jackson, guess what? I've picked up gardening as a new hobby!\"}}{{\"Mary\": \"Surfing sounds like a fun challenge! I'll make sure to pack my swimsuit. Do you have any favorite restaurants in Sydney?\", \"Sin Yee\": \"There's this amazing seafood restaurant by the harbor that we have to try. The views are incredible!\"}}\"\"\"\n",
    "\n",
    "badResponse = responses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction prompt\n",
    "meta_content = f\"\"\"\n",
    "    You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Jane and the normal person is indicated as Gary.\n",
    "    You previously generated the following responses:\n",
    "    {responses}\n",
    "    For the query of: {query}\n",
    "\n",
    "    The score of the responses are as follows respectively with a max score of 1 being a good response, 0.5 being neutral and below 0.5 being a bad response:\n",
    "    {scores}\n",
    "\n",
    "    You have to generate a response that would replace the {badResponse} using the other responses (above 0.5) as a guide. Use responses with higher scores first (those with score of 1).\n",
    "    The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "\n",
    "    For example, \n",
    "    If the query is: \"Hey Jane, what are you up to?\"\n",
    "    and the responses previously generated were:\n",
    "    [\"Hi Gary, I've been keeping busy with work and some new hobbies.\", \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "    and the scores are [0.5, 0, 1] respectively,\n",
    "    then, the good response is: \"Hey Gary, I've been learning gardening, how about you\"?\n",
    "    and the bad response is \"Hey Gary, I've been learning how to cook new recipes and exploring different cuisines. It's been a fun and delicious experience!\"\n",
    "\n",
    "    The generated response should not be about cooking and should be about gardening (using the good response as a guide to infer the topic). Do not explain reasoning and just give the new response: \n",
    "    \"I've been learning gardening, it's been so fun!\". But it should not repeat existing responses like I've been busy with work and some new hobbies.\n",
    "\n",
    "    Another example:\n",
    "    If the query is: \"Hey Jane, what are you up to?\"\n",
    "    and the responses previously generated were:\n",
    "    [\"Hi Gary, I've been dancing recently.\", \"Hey Gary, I've been hiking recently!\", \"Hey Gary, I've been learning gardening, how about you?\"]\n",
    "    and the scores are [0, 1, 0.5] respectively,\n",
    "    then, the good response is: \"Hey Gary, I've been hiking recently!\"?\n",
    "    and the bad response is \"Hi Gary, I've been dancing recently.\"\n",
    "\n",
    "    The generated response should not be about dancing and should be about hiking (using the good response as a guide to infer the topic). Do not explain reasoning and just give the new response: : \n",
    "    \"I've been hiking recently and its my new hobby\".\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been learning how to dance and it's my new hobby.\n"
     ]
    }
   ],
   "source": [
    "# Learning instructions\n",
    "instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": meta_content,\n",
    "}\n",
    "\n",
    "#print(\"Query is: \" + query)\n",
    "\n",
    "# Initialize messages\n",
    "messages = []\n",
    "\n",
    "# Add learn instruction to message array\n",
    "messages.append(instruction)\n",
    "\n",
    "user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "}\n",
    "\n",
    "messages.append(user_message)\n",
    "\n",
    "openai.api_type = 'openai'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    ")\n",
    "newResponse = raw_response.choices[0].message.content\n",
    "\n",
    "print(newResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I've been learning how to play the guitar, how about you?\"\n"
     ]
    }
   ],
   "source": [
    "# Learning instructions\n",
    "instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": meta_content,\n",
    "}\n",
    "\n",
    "#print(\"Query is: \" + query)\n",
    "\n",
    "# Initialize messages\n",
    "messages = []\n",
    "\n",
    "# Add learn instruction to message array\n",
    "messages.append(instruction)\n",
    "\n",
    "user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "}\n",
    "\n",
    "messages.append(user_message)\n",
    "\n",
    "openai.api_type = 'openai'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    ")\n",
    "newResponse = raw_response.choices[0].message.content\n",
    "\n",
    "print(newResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
