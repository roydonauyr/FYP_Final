{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "#!pip install python-dotenv\n",
    "#!pip install langchain\n",
    "#!pip install langchain_openai\n",
    "#!pip install langchain-core\n",
    "#!pip install langchain_text_splitters\n",
    "#!pip install langchain_community\n",
    "#!pip install -qU langchain-community faiss-cpu\n",
    "#!pip install decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from time import sleep\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = f\"\"\"\n",
    "{os.environ['AZURE_OPENAI_ENDPOINT']}\n",
    "{os.environ['AZURE_OPENAI_APIKEY']}\n",
    "{os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']}\n",
    "{os.environ['AZURE_OPENAI_MODEL_NAME']}\n",
    "{os.environ['AZURE_OPENAI_API_VERSION']}\n",
    "\"\"\"\n",
    "#print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print as Object: \n",
      " content='Life is short, make the most of every moment given.' response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_e49e4201a9', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-ea1061f4-f34c-4309-b8a0-f0e1f4220b17-0' usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25}\n",
      "========================================\n",
      "Print as JSON: \n",
      " {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Life is short, make the most of every moment given.', 'response_metadata': {'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_e49e4201a9', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, 'type': 'ai', 'id': 'run-ea1061f4-f34c-4309-b8a0-f0e1f4220b17-0', 'usage_metadata': {'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25}, 'tool_calls': [], 'invalid_tool_calls': []}}\n",
      "========================================\n",
      "Print content only: \n",
      " Life is short, make the most of every moment given.\n"
     ]
    }
   ],
   "source": [
    "# Instruction prompt\n",
    "name = \"Roydon\"\n",
    "persona = f\"\"\"You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is {name} and the normal person is indicated as other person.\"\"\"\n",
    "task = \"\"\" You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \n",
    "        The topic should be interpreted from the conversation.\n",
    "        If no topic could be interpreted, provide default responses that a person would start with such as greetings. \n",
    "        The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "        In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\n",
    "        Ensure the responses generated will allow the conversation to flow smoothly.\"\"\"\n",
    "context = \"\"\n",
    "condition = \"\"\"It must be in english. An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.\"\"\"\n",
    "\n",
    "# Construct message object\n",
    "instruction = f\"{persona} {task} {condition}\"\n",
    "messages = [SystemMessage(content=instruction)]\n",
    "\n",
    "\n",
    "response = llm.invoke(\"Say something in 10 words\")\n",
    "\n",
    "## OUTPUT\n",
    "print(\"Print as Object: \\n\",response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print as JSON: \\n\",response.to_json())\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human : hi how have you been\n",
      "\n",
      "AI    : Response 1: I've been good, thanks for asking. Response 2: Not too bad, just keeping busy. Response 3: I've been doing well, how about you?\n",
      "\n",
      "Human : hi\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1019\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1019\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://roydon-fyp-model-1.openai.azure.com//openai/deployments/FYP-azureopenai-gpt35-usnorth/chat/completions?api-version=2024-06-01'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m usermsg \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m      9\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend(usermsg)\n\u001b[1;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI    : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    283\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    750\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    755\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:613\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    612\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    614\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    615\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    617\u001b[0m ]\n\u001b[0;32m    618\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:603\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 603\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m         )\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 825\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:635\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 635\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1024\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1072\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1068\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m-> 1072\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1076\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1080\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prompt Engineered\n",
    "\n",
    "#print(instruction)\n",
    "query = input(\"Enter your message: \")\n",
    "\n",
    "while query != \"exit\":\n",
    "    print(f\"Human : {query}\\n\")\n",
    "    usermsg = HumanMessage(content=query)\n",
    "    messages.append(usermsg)\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    sleep(1)\n",
    "    print(f\"AI    : {response.content}\\n\")\n",
    "    messages.append(response)\n",
    "\n",
    "    ## re-prompt\n",
    "    query = input(\"Enter your message (enter exit to end): \")\n",
    "\n",
    "print(\"Conversation ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Conversation Log\n",
      "[SystemMessage(content='You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Roydon and the normal person is indicated as other person.  You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \\n        The topic should be interpreted from the conversation.\\n        If no topic could be interpreted, provide default responses that a person would start with such as greetings. \\n        The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\\n        In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\\n        Ensure the responses generated will allow the conversation to flow smoothly. It must be in english. An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.'), HumanMessage(content='hi how have you been'), AIMessage(content=\"Response 1: I've been good, thanks for asking. Response 2: Not too bad, just keeping busy. Response 3: I've been doing well, how about you?\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 250, 'total_tokens': 289}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_e49e4201a9', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-45563707-efc0-44e4-a3be-db02f9b8c14f-0', usage_metadata={'input_tokens': 250, 'output_tokens': 39, 'total_tokens': 289}), HumanMessage(content='hi')]\n"
     ]
    }
   ],
   "source": [
    "# Print Chatlog\n",
    "print(\"Final Conversation Log\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating json file mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human : The person is Xavier. The topic is on Justin arranging a badminton match with Xavier.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample declaration: The person is Roydon, with a personality of optimistic. The topic is on football and he is an Arsenal Fan \n",
    "# Lee Hang, Xavier, Matt, Jackson, Cheryl, Mary\n",
    "\"The person is . The topic is on .\"\n",
    "\n",
    "# Instruction prompt\n",
    "persona = \"\"\"You would be assisting in creating mock data to be used\"\"\"\n",
    "task = \"\"\" I am trying to create mock data of conversations between 2 people. I will provide you with the topic and the persons personality. Generate replies with another person based on that.\n",
    "\"\"\"\n",
    "condition = \"\"\"You would be generating a conversation of at least 40 different back and forth replies for the person Justin. The replies generated should be linked and flow smoothly. Justin should always be the second person replying.\n",
    "For example, Xavier says: How have nursing been? Justin replies: Its been really busy but fulfilling.\n",
    "Xavier says: Well why the sudden increase in workload? Justin: Due to covid19, we have been short staffed and the number of patients keeps increasing.\"\"\"\n",
    "#context = f\"Person name is {name}, with a personality of {personality}. The topic is on {topic}\"\n",
    "\n",
    "# Construct message object\n",
    "instruction = f\"{persona} {task} {condition}\"\n",
    "messages = [SystemMessage(content=instruction)]\n",
    "\n",
    "query = input(\"Enter your message: \")\n",
    "\n",
    "print(f\"Human : {query}\\n\")\n",
    "usermsg = HumanMessage(content=query)\n",
    "messages.append(usermsg)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Parsing the conversation\n",
    "lines = response.content.split('\\n')\n",
    "responses = {}\n",
    "response_index = 1  # To number each response pair\n",
    "\n",
    "# Iterate over lines in pairs\n",
    "for i in range(0, len(lines), 2):\n",
    "    response_key = f\"Response {response_index}\"\n",
    "    responses[response_key] = {}\n",
    "\n",
    "    # Process first speaker\n",
    "    if i < len(lines) and \": \" in lines[i]:\n",
    "        speaker, message = lines[i].split(\": \", 1)\n",
    "        responses[response_key][speaker] = message.strip()\n",
    "\n",
    "    # Process second speaker, if exists\n",
    "    if i + 1 < len(lines) and \": \" in lines[i + 1]:\n",
    "        speaker, message = lines[i + 1].split(\": \", 1)\n",
    "        responses[response_key][speaker] = message.strip()\n",
    "\n",
    "    response_index += 1\n",
    "\n",
    "with open('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Justin\\\\Xavier.json', 'a') as f:\n",
    "    json.dump(responses, f, indent=4)\n",
    "    f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir('C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\mockdata\\\\'):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(f'C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\mockdata\\\\{filename}') as f:\n",
    "            data = json.load(f)\n",
    "            for response_label, conversation in data.items():\n",
    "                doc_content = json.dumps(conversation)\n",
    "                doc_metadata = {\"label\": response_label, \"source\": filename}\n",
    "                documents.append(Document(page_content=doc_content, metadata=doc_metadata))\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Roydon\": \"Hey there! Can\\'t wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Roydon! Yeah, it\\'s always exciting to see how your team will perform. Optimistic as always, I see!\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_to_dict(json_string):\n",
    "    return json.loads(json_string)\n",
    "\n",
    "def convert_dict_to_text(conversation_dict):\n",
    "    text = \"\"\n",
    "    for speaker, message in conversation_dict.items():\n",
    "        text += f\"{speaker}: {message}\\n\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roydon: Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\n",
      "\n",
      "John: Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!\n"
     ]
    }
   ],
   "source": [
    "test = convert_dict_to_text(parse_json_to_dict(documents[0].page_content))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Document: Roydon: Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\n",
      "\n",
      "John: Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!\n",
      "Chunked Document: Roydon: Absolutely! Gotta stay positive, right? What team do you support?\n",
      "\n",
      "John: I'm a Manchester United fan, so we might have some friendly rivalry this season!\n",
      "Chunked Document: Roydon: Haha, definitely! Looking forward to some intense matches between our teams. Who do you think will be Arsenal's key player this season?\n",
      "\n",
      "John: I have a feeling Aubameyang will continue to shine for Arsenal. His goals are always a game-changer!\n",
      "Chunked Document: Roydon: I couldn't agree more! Aubameyang is a true asset to the team. Do you think Arsenal will make it to the top four this season?\n",
      "\n",
      "John: It's definitely possible with the right strategy and teamwork. What are your thoughts on Arsenal's new signings?\n",
      "Chunked Document: Roydon: I'm optimistic about the new signings, especially Ben White. I think he'll strengthen our defense. How about Manchester United's signings?\n",
      "\n",
      "John: Varane and Sancho are exciting additions to our squad. Hoping they make a big impact this season. Do you think Arsenal's manager is the right fit for the team?\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "# Set up the text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Process and chunk each document\n",
    "chunked_documents = []\n",
    "for document in documents:\n",
    "    conversation_dict = parse_json_to_dict(document.page_content)\n",
    "    conversation_text = convert_dict_to_text(conversation_dict)\n",
    "    chunks = splitter.split_text(conversation_text)\n",
    "    chunked_documents.extend(chunks)\n",
    "\n",
    "# Optionally, print out some chunked documents to verify\n",
    "for doc in chunked_documents[:5]:  # Print first 5 chunked documents\n",
    "    print(\"Chunked Document:\", doc)\n",
    "\n",
    "print(len(chunked_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])\n",
    "\n",
    "#embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"Roydon\": \"Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!\"}' metadata={'label': 'Response 1', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"Absolutely! Gotta stay positive, right? What team do you support?\", \"John\": \"I'm a Manchester United fan, so we might have some friendly rivalry this season!\"}' metadata={'label': 'Response 2', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"Haha, definitely! Looking forward to some intense matches between our teams. Who do you think will be Arsenal's key player this season?\", \"John\": \"I have a feeling Aubameyang will continue to shine for Arsenal. His goals are always a game-changer!\"}' metadata={'label': 'Response 3', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"I couldn't agree more! Aubameyang is a true asset to the team. Do you think Arsenal will make it to the top four this season?\", \"John\": \"It's definitely possible with the right strategy and teamwork. What are your thoughts on Arsenal's new signings?\"}' metadata={'label': 'Response 4', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"I'm optimistic about the new signings, especially Ben White. I think he'll strengthen our defense. How about Manchester United's signings?\", \"John\": \"Varane and Sancho are exciting additions to our squad. Hoping they make a big impact this season. Do you think Arsenal's manager is the right fit for the team?\"}' metadata={'label': 'Response 5', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"Arteta has been making some good decisions lately, so I have faith in him. How about Ole Gunnar Solskjaer?\", \"John\": \"Solskjaer has been improving as a manager, but there's still room for growth. It'll be interesting to see how both managers perform this season. What's your prediction for Arsenal's first match?\"}' metadata={'label': 'Response 6', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"I'm predicting a 2-1 win for Arsenal! What about Manchester United's first match?\", \"John\": \"I think we'll start strong with a 3-0 win. It's going to be an exciting season for both our teams!\"}' metadata={'label': 'Response 7', 'source': 'football.json', 'file_name': 'football.json'}\n",
      "page_content='{\"Roydon\": \"Hey there! Did you catch the Arsenal game last night? What a thrilling match!\", \"John\": \"Hey Roydon! Yes, I watched it. Arsenal played really well, didn't they?\"}' metadata={'label': 'Response 1', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Absolutely! I love how they never gave up, even when they were behind. That's the spirit!\", \"John\": \"Yeah, their determination is truly inspiring. It's great to see them improving.\"}' metadata={'label': 'Response 2', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Definitely! I have faith that they will continue to get better and make us proud.\", \"John\": \"I agree. It's exciting to think about what they can achieve in the upcoming matches.\"}' metadata={'label': 'Response 3', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"I can't wait to see them play again. Supporting Arsenal always brings me so much joy.\", \"John\": \"That's the spirit, Roydon! Your optimism is contagious. Arsenal is lucky to have fans like you.\"}' metadata={'label': 'Response 4', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Thanks, John! I believe in staying positive no matter what. It's the best way to support the team.\", \"John\": \"Absolutely, positivity goes a long way. I'm sure Arsenal feels the love from fans like you.\"}' metadata={'label': 'Response 5', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"I hope so! Cheering them on and believing in their potential is what being a fan is all about.\", \"John\": \"Well said, Roydon. Let's keep spreading that positivity and cheering for Arsenal together!\"}' metadata={'label': 'Response 6', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Definitely, John! We'll be the loudest supporters in the stands, no doubt about it.\", \"John\": \"Haha, I have no doubt about that! Arsenal is lucky to have fans like us backing them up.\"}' metadata={'label': 'Response 7', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"That's right! We'll always stand by our team, through thick and thin. Go Gunners!\", \"John\": \"Go Gunners indeed! Here's to many more victories and exciting matches ahead. Arsenal forever!\"}' metadata={'label': 'Response 8', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Cheers to that, John! Arsenal forever in our hearts. Let's keep the faith and keep cheering them on!\"}' metadata={'label': 'Response 9', 'source': 'football2.json', 'file_name': 'football2.json'}\n",
      "page_content='{\"Roydon\": \"Hey Yas, I'm still fuming about my terrible experience in Thailand, but you know what? I'm super excited about my upcoming trip to Japan!\", \"Yas\": \"Oh no, what happened in Thailand? But that's great to hear about Japan! What are you looking forward to the most?\"}' metadata={'label': 'Response 1', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Well, everything that could go wrong in Thailand did go wrong. But in Japan, I can't wait to explore the beautiful temples and try authentic Japanese cuisine.\", \"Yas\": \"That sounds like a rough time in Thailand, but Japan sounds amazing! Have you planned out your itinerary yet?\"}' metadata={'label': 'Response 2', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Not yet, but I'm thinking of visiting Tokyo, Kyoto, and Osaka. I want to experience both the bustling city life and the serene countryside.\", \"Yas\": \"That sounds like a perfect balance! I'm sure you'll have a fantastic time exploring all those places.\"}' metadata={'label': 'Response 3', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"I hope so! I've heard so many great things about Japan, and I'm determined to make the most of this trip.\", \"Yas\": \"Your positive attitude will surely make this trip unforgettable. I can't wait to hear all about your adventures when you get back!\"}' metadata={'label': 'Response 4', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Thanks, Yas! I'm going into this trip with an open mind and a heart full of optimism. I know Japan will not disappoint.\", \"Yas\": \"Absolutely, positivity attracts positivity! I'm sure Japan will welcome you with open arms and unforgettable experiences.\"}' metadata={'label': 'Response 5', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"I couldn't agree more! I'm ready to leave all the negativity from Thailand behind and embrace the beauty and culture of Japan.\", \"Yas\": \"That's the spirit, Roydon! Japan is waiting for you with open arms, ready to show you the best it has to offer.\"}' metadata={'label': 'Response 6', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"I can't wait to immerse myself in everything Japan has to offer and create lasting memories that will overshadow my Thailand trip.\", \"Yas\": \"Your positive outlook will surely make this trip one for the books! Japan is lucky to have you as a visitor.\"}' metadata={'label': 'Response 7', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Thank you, Yas! I'm grateful for the opportunity to experience Japan and create new memories that will last a lifetime.\", \"Yas\": \"Cherish every moment and embrace the journey with open arms. Japan is about to become your new favorite destination!\"}' metadata={'label': 'Response 8', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"I'm ready to embrace all that Japan has to offer and make the most of this incredible opportunity. I'm leaving all negativity behind and focusing on the beauty ahead.\", \"Yas\": \"Your optimism is truly inspiring, Roydon! Japan is in for a treat with your positive energy and open heart.\"}' metadata={'label': 'Response 9', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Thank you, Yas! I believe that positivity is the key to unlocking amazing experiences, and I'm ready to unlock the wonders of Japan.\", \"Yas\": \"With your positive mindset, Japan will surely exceed all your expectations and leave you with unforgettable memories. Safe travels, Roydon!\"}' metadata={'label': 'Response 10', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"Thank you, Yas! I'll make sure to savor every moment and embrace the journey with optimism and gratitude. Japan, here I come!\"}' metadata={'label': 'Response 11', 'source': 'next_trip.json', 'file_name': 'next_trip.json'}\n",
      "page_content='{\"Roydon\": \"I can't believe what happened to me in Thailand.\", \"Xavier\": \"What happened?\"}' metadata={'label': 'Response 1', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I got scammed by a taxi driver and lost all my money.\", \"Xavier\": \"That's terrible! Did you report it to the authorities?\"}' metadata={'label': 'Response 2', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I tried, but they couldn't do anything about it.\", \"Xavier\": \"I'm so sorry to hear that. Did you manage to get any help?\"}' metadata={'label': 'Response 3', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"No, I had to borrow money from a friend to get back home.\", \"Xavier\": \"That must have been a really stressful experience.\"}' metadata={'label': 'Response 4', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"It was the worst trip of my life.\", \"Xavier\": \"Hopefully, things will get better from here on out.\"}' metadata={'label': 'Response 5', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I hope so too, but I'm still so angry about what happened.\", \"Xavier\": \"It's completely understandable to feel that way.\"}' metadata={'label': 'Response 6', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I just can't believe people would take advantage of others like that.\", \"Xavier\": \"Unfortunately, there are dishonest people everywhere.\"}' metadata={'label': 'Response 7', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I'll never forget this experience, that's for sure.\", \"Xavier\": \"It's a tough lesson to learn, but hopefully, it will make you more cautious in the future.\"}' metadata={'label': 'Response 8', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"I'll definitely be more careful when traveling from now on.\", \"Xavier\": \"That's a good mindset to have. Stay safe out there, Roydon.\"}' metadata={'label': 'Response 9', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"Thanks, I appreciate your support.\", \"Xavier\": \"Anytime, buddy. Take care.\"}' metadata={'label': 'Response 10', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"You too, Xavier.\"}' metadata={'label': 'Response 11', 'source': 'next_trip2.json', 'file_name': 'next_trip2.json'}\n",
      "page_content='{\"Roydon\": \"Guess what, I just got a new pet dog!\", \"Jacob\": \"That's awesome! What breed is it?\"}' metadata={'label': 'Response 1', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"It's a golden retriever, and he's the cutest thing ever!\", \"Jacob\": \"Golden retrievers are so friendly and loyal, you're going to have so much fun with him!\"}' metadata={'label': 'Response 2', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"I can't wait to take him for walks in the park and play fetch with him.\", \"Jacob\": \"He's going to love all the attention and exercise, you'll have a new best friend in no time!\"}' metadata={'label': 'Response 3', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"I already feel like we have a special bond, he follows me everywhere I go.\", \"Jacob\": \"That's the best feeling, knowing that your dog trusts and loves you unconditionally.\"}' metadata={'label': 'Response 4', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"I feel like my heart is so full having him around, he brings so much joy into my life.\", \"Jacob\": \"Dogs have a way of filling our lives with happiness and love, it's truly a special connection.\"}' metadata={'label': 'Response 5', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"I couldn't agree more, I feel like my new dog has completed my little family.\", \"Jacob\": \"It's amazing how pets have a way of making a house feel like a home, enjoy every moment with your furry friend!\"}' metadata={'label': 'Response 6', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Jacob\": \"I definitely will, I can't imagine my life without him now.\"}' metadata={'label': 'Response 7', 'source': 'pet.json', 'file_name': 'pet.json'}\n",
      "page_content='{\"Roydon\": \"I can't wait to take my new golden retriever dog on long hikes in the mountains!\", \"Xavier\": \"That sounds like a great idea! Golden retrievers love the outdoors. Have you thought about teaching him any tricks?\"}' metadata={'label': 'Response 1', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"Absolutely! I'm planning to teach him how to fetch a frisbee and maybe even do some agility training.\", \"Xavier\": \"That's awesome! I'm sure your golden retriever will excel at those activities. Have you thought about taking him to the beach?\"}' metadata={'label': 'Response 2', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"Yes, I definitely want to take him to the beach! I can already imagine him running around in the sand and splashing in the waves.\", \"Xavier\": \"That sounds like a perfect day out for both of you! Have you considered enrolling him in any obedience classes?\"}' metadata={'label': 'Response 3', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"Yes, I think it would be a great idea to enroll him in obedience classes. It will help strengthen our bond and improve his behavior.\", \"Xavier\": \"That's a smart move! Training together will definitely strengthen your relationship. Do you have any other fun activities planned for you and your golden retriever?\"}' metadata={'label': 'Response 4', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"I'm thinking about organizing playdates with other dogs in the neighborhood. It will be a great way for him to socialize and make new friends.\", \"Xavier\": \"That's a fantastic idea! Socializing is important for dogs, and it will also be a great opportunity for you to meet other dog owners. Have you thought about documenting your adventures with your golden retriever?\"}' metadata={'label': 'Response 5', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"Yes, I plan to create a special Instagram account just for him! I want to share our adventures and cute moments with everyone.\", \"Xavier\": \"That's a brilliant idea! I'm sure your golden retriever will become an Instagram star in no time. Do you have a name picked out for him yet?\"}' metadata={'label': 'Response 6', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"I'm thinking of naming him Sunny, because he brings so much joy and sunshine into my life. What do you think?\", \"Xavier\": \"Sunny is a perfect name for a golden retriever! It suits his happy and cheerful personality. I can't wait to see all the fun adventures you two will have together.\"}' metadata={'label': 'Response 7', 'source': 'pet2.json', 'file_name': 'pet2.json'}\n",
      "page_content='{\"Roydon\": \"I can't believe how terrible my trip to Thailand was. Everything went wrong.\", \"Dory\": \"Oh no, what happened? Did you miss your flight?\"}' metadata={'label': 'Response 1', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"No, it was worse than that. The hotel lost my reservation and I had to sleep on the streets.\", \"Dory\": \"That sounds awful. Did you at least get to see some sights during the day?\"}' metadata={'label': 'Response 2', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"I tried to, but everywhere I went, I just kept getting ripped off by the locals.\", \"Dory\": \"That must have been frustrating. Did you try any of the street food at least?\"}' metadata={'label': 'Response 3', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"I did, but I got food poisoning and spent the rest of the trip in the hospital.\", \"Dory\": \"That's terrible luck. Did you manage to salvage any part of the trip at all?\"}' metadata={'label': 'Response 4', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"Not really. I was so angry the whole time, I couldn't enjoy anything.\", \"Dory\": \"I'm sorry to hear that, Roydon. Maybe you can plan a better trip next time.\"}' metadata={'label': 'Response 5', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"I don't think I ever want to travel again. It's just not worth the hassle.\", \"Dory\": \"I understand how you feel, but don't let one bad experience ruin it for you forever.\"}' metadata={'label': 'Response 6', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"Easy for you to say. You weren't the one stuck in a foreign country with nothing going right.\", \"Dory\": \"I know, but sometimes these things happen. You just have to try and make the best of it.\"}' metadata={'label': 'Response 7', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"I'll try to remember that for next time, but right now, I just need to vent about how awful it was.\", \"Dory\": \"Vent all you need to, Roydon. I'm here to listen and support you through it all.\"}' metadata={'label': 'Response 8', 'source': 'travel.json', 'file_name': 'travel.json'}\n",
      "page_content='{\"Roydon\": \"Thanks, Dory. I appreciate you being there for me, even when I'm at my angriest.\"}' metadata={'label': 'Response 9', 'source': 'travel.json', 'file_name': 'travel.json'}\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    doc.metadata['file_name'] = doc.metadata['source']\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x204d3901150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_vectorstore_v3 = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2534ee60850>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_vectorstore_text = FAISS.from_texts(chunked_documents, embeddings)\n",
    "\n",
    "faiss_vectorstore_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Vector Store\n",
    "faiss_vectorstore_v3.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs_v3\")\n",
    "\n",
    "## Saving Vector Store Text (Testing)\n",
    "#faiss_vectorstore_text.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vector Store & Context Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])\n",
    "\n",
    "loaded_faiss_vs = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "loaded_faiss_vs_text = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs_text\", embeddings=embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== \n",
      " {'label': 'Response 1', 'source': 'football2.json'}\n",
      "{\"Roydon\": \"Hey there! Did you catch the Arsenal game last night? What a thrilling match!\", \"John\": \"Hey Roydon! Yes, I watched it. Arsenal played really well, didn't they?\"}\n",
      "============================== \n",
      " {'label': 'Response 1', 'source': 'football.json'}\n",
      "{\"Roydon\": \"Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!\"}\n",
      "============================== \n",
      " {'label': 'Response 1', 'source': 'pet.json'}\n",
      "{\"Roydon\": \"Guess what, I just got a new pet dog!\", \"Jacob\": \"That's awesome! What breed is it?\"}\n",
      "============================== \n",
      " {'label': 'Response 9', 'source': 'travel.json'}\n",
      "{\"Roydon\": \"Thanks, Dory. I appreciate you being there for me, even when I'm at my angriest.\"}\n",
      "============================== \n",
      " {'label': 'Response 2', 'source': 'football.json'}\n",
      "{\"Roydon\": \"Absolutely! Gotta stay positive, right? What team do you support?\", \"John\": \"I'm a Manchester United fan, so we might have some friendly rivalry this season!\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"How have you been Roydon, what are you up to?\"\n",
    "context = loaded_faiss_vs.similarity_search(query, k=5)\n",
    "\n",
    "for con in context:\n",
    "    print(\"=\" * 30, \"\\n\", con.metadata)\n",
    "    print(con.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== \n",
      "\n",
      "Roydon: Hey there! Did you catch the Arsenal game last night? What a thrilling match!\n",
      "\n",
      "John: Hey Roydon! Yes, I watched it. Arsenal played really well, didn't they?\n",
      "============================== \n",
      "\n",
      "Roydon: Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\n",
      "\n",
      "John: Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!\n",
      "============================== \n",
      "\n",
      "Roydon: Guess what, I just got a new pet dog!\n",
      "\n",
      "Jacob: That's awesome! What breed is it?\n",
      "============================== \n",
      "\n",
      "Roydon: Hey Yas, I'm still fuming about my terrible experience in Thailand, but you know what? I'm super excited about my upcoming trip to Japan!\n",
      "\n",
      "Yas: Oh no, what happened in Thailand? But that's great to hear about Japan! What are you looking forward to the most?\n",
      "============================== \n",
      "\n",
      "Roydon: Thanks, Dory. I appreciate you being there for me, even when I'm at my angriest.\n"
     ]
    }
   ],
   "source": [
    "query = \"How have you been Roydon, what are you up to?\"\n",
    "context = loaded_faiss_vs_text.similarity_search(query, k=5)\n",
    "\n",
    "for con in context:\n",
    "    print(\"=\" * 30, \"\\n\")\n",
    "    print(con.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the RAG System Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m## Retrieve and consolidate context based on query\u001b[39;00m\n\u001b[0;32m     24\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow have you been Roydon?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m loaded_faiss_vs \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mRoydon\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGithub\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFYP_Application\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMuteCompanion\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvector_store\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvectorstores\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfaiss_vs\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings\u001b[38;5;241m=\u001b[39m\u001b[43membeddings\u001b[49m, allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m context \u001b[38;5;241m=\u001b[39m loaded_faiss_vs\u001b[38;5;241m.\u001b[39msimilarity_search(query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     27\u001b[0m contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# PROMPT Engineering\n",
    "name = \"Roydon\"\n",
    "other_person = \"Jacob\"\n",
    "persona = f\"\"\"You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is {name} and the normal person is indicated as {other_person}.\"\"\"\n",
    "task = \"\"\" You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \n",
    "        The topic should be interpreted from the conversation.\n",
    "        If no topic could be interpreted, provide default responses that a person would start with such as greetings. \n",
    "        The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "        In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\n",
    "        Ensure the responses generated will allow the conversation to flow smoothly.\"\"\"\n",
    "condition = \"\"\"It must be in english. An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.\"\"\"\n",
    "\n",
    "# Construct message object\n",
    "instruction = f\"{persona} {task} {condition}\"\n",
    "messages = [SystemMessage(content=instruction)]\n",
    "\n",
    "## Retrieve and consolidate context based on query\n",
    "query = \"How have you been Roydon?\"\n",
    "loaded_faiss_vs = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "context = loaded_faiss_vs.similarity_search(query, k=3)\n",
    "contexts = \"\"\n",
    "\n",
    "for con in context:\n",
    "    contexts += con.page_content\n",
    "\n",
    "context_query = f\"\"\"\n",
    "    Please generate the responses for {name} based on the following context provided below.\\n\n",
    "    Context: {contexts}\n",
    "    Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "## Append the context and user query to chatlog\n",
    "messages.append(HumanMessage(content=context_query))\n",
    "\n",
    "answer = llm.invoke(messages)\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall decouple\n",
    "#!pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config' from 'decouple' (c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\decouple\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecouple\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m      8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m AzureOpenAIEmbeddings(azure_endpoint\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      9\u001b[0m                                    api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_APIKEY\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     10\u001b[0m                                    model\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT_EMBEDDING_MODEL_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m                                    azure_deployment\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT_EMBEDDING_DEPLOYMENT_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Environment variables\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'config' from 'decouple' (c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\decouple\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from decouple import config\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])\n",
    "\n",
    "# Environment variables\n",
    "openai.organisation = config(\"OPEN_AI_ORG\")\n",
    "openai.api_key = config(\"OPEN_AI_KEY\")\n",
    "\n",
    "# PROMPT Engineering\n",
    "name = \"Roydon\"\n",
    "other_person = \"Jacob\"\n",
    "persona = f\"\"\"You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is {name} and the normal person is indicated as {other_person}.\"\"\"\n",
    "task = \"\"\" You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \n",
    "        The topic should be interpreted from the conversation.\n",
    "        If no topic could be interpreted, provide default responses that a person would start with such as greetings. \n",
    "        The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "        In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\n",
    "        Ensure the responses generated will allow the conversation to flow smoothly.\"\"\"\n",
    "condition = \"\"\"It must be in english. An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.\"\"\"\n",
    "\n",
    "# Construct message object\n",
    "instruction = f\"{persona} {task} {condition}\"\n",
    "messages = [SystemMessage(content=instruction)]\n",
    "\n",
    "## Retrieve and consolidate context based on query\n",
    "query = \"How have you been Roydon?\"\n",
    "loaded_faiss_vs = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "context = loaded_faiss_vs.similarity_search(query, k=3)\n",
    "contexts = \"\"\n",
    "\n",
    "for con in context:\n",
    "    contexts += con.page_content\n",
    "\n",
    "context_query = f\"\"\"\n",
    "    Please generate the responses for {name} based on the following context provided below.\\n\n",
    "    Context: {contexts}\n",
    "    Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "## Append the context and user query to chatlog\n",
    "messages.append(HumanMessage(content=context_query))\n",
    "\n",
    "raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages \n",
    ")\n",
    "\n",
    "response_choices = raw_response.choices[0].message.content\n",
    "\n",
    "print(response_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and deleting from vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage using open ai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs\", embeddings=embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('257c5c24-764e-4535-bc5d-f2ff0e616dc8')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_response = \"\"\"{\"John\": \"testing out\", \"Roydon\": \"testing testing\"}\"\"\"\n",
    "response_label = \"Response 1\"\n",
    "filename = \"current_conversation.json\"\n",
    "doc_metadata = {\"label\": response_label, \"source\": filename,  'file_name': filename}\n",
    "\n",
    "response_document = Document(page_content=vectorized_response, metadata=doc_metadata)\n",
    "\n",
    "documents = [response_document]\n",
    "ids = [uuid4()]\n",
    "\n",
    "loaded_faiss_vs.add_documents(documents=documents, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Query directly\n",
    "results = loaded_faiss_vs.similarity_search(\n",
    "    \"\",\n",
    "    k=1,\n",
    "    filter={\"source\": \"current_conversation.json\"},\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_faiss_vs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Delete the document:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mloaded_faiss_vs\u001b[49m\u001b[38;5;241m.\u001b[39mremove_ids(ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me2846e7a-6e9c-4beb-9a2c-259d7eb41fb6\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_faiss_vs' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete the document:\n",
    "loaded_faiss_vs.delete(ids=['e2846e7a-6e9c-4beb-9a2c-259d7eb41fb6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Query directly\n",
    "results = loaded_faiss_vs.similarity_search(\n",
    "    \"\"\"{\"John\": \"testing out\", \"Roydon\": \"testing testing\"}\"\"\",\n",
    "    k=1,\n",
    "    filter={\"source\": filename},\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage using hugging face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load in environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs_hf_v1 = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v1\", embeddings=embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('0a881c8a-9a92-47e6-9a67-13b9c9be1ba2')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_response = \"\"\"{\"John\": \"testing out\", \"Roydon\": \"testing testing\"}\"\"\"\n",
    "response_label = \"Response 2\"\n",
    "filename = \"current_conversation2.json\"\n",
    "id = uuid4()\n",
    "doc_metadata = {\"label\": response_label, \"source\": filename,  'file_name': filename, 'id': id}\n",
    "\n",
    "\n",
    "response_document = Document(page_content=vectorized_response, metadata=doc_metadata)\n",
    "\n",
    "documents = [response_document]\n",
    "ids = [id]\n",
    "\n",
    "loaded_faiss_vs_hf_v1.add_documents(documents=documents, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs_hf_v1.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'label': 'Response 2', 'source': 'current_conversation2.json', 'file_name': 'current_conversation2.json', 'id': UUID('0a881c8a-9a92-47e6-9a67-13b9c9be1ba2')}, page_content='{\"John\": \"testing out\", \"Roydon\": \"testing testing\"}')]\n"
     ]
    }
   ],
   "source": [
    "# Query directly\n",
    "results = loaded_faiss_vs_hf_v1.similarity_search(\n",
    "    \"\",\n",
    "    k=1,\n",
    "    filter={\"source\": \"current_conversation2.json\"},\n",
    ")\n",
    "\n",
    "print(results)\n",
    "result_id = results[0].metadata['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the index: 63\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vectors in the index:\", loaded_faiss_vs_hf_v1.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '27feeac1-b036-4502-9dd6-a01e407479e0',\n",
       " 1: 'eae5c2b7-6471-4d6e-85b5-db5cf4ece23d',\n",
       " 2: '6cac4353-5b95-4e3f-b553-155daa01a3d3',\n",
       " 3: 'd74fa78e-aa24-4279-8aeb-f21857cab3cf',\n",
       " 4: '20387c02-ab8a-46ac-9bc5-3b7183ec843b',\n",
       " 5: '88902551-f406-4904-bc44-eae2d43b1f1a',\n",
       " 6: '10743b0d-b3d4-4760-be5a-cf7a619e30c9',\n",
       " 7: '635bb1f3-d102-44db-896c-52dd00aceac8',\n",
       " 8: '8c4a3c53-8650-4195-b226-e95ea30f2d32',\n",
       " 9: '74b1e529-7219-4918-bf58-cdab5f91f848',\n",
       " 10: '04b13d19-5a08-4465-8161-c5b12bfaeba3',\n",
       " 11: '9a2e199f-d36e-47b1-8dbf-7d279e605501',\n",
       " 12: '6d10f550-1b12-4630-9847-2f582e693347',\n",
       " 13: '05d3a4af-cdbc-4d77-9201-b3a788fdbb02',\n",
       " 14: '6dc1b973-d864-4fc6-92bc-2a04139c9e07',\n",
       " 15: '77ba02a6-2d3b-41b0-8e91-0d1bae85957d',\n",
       " 16: 'e74ceaac-942e-4a10-b9b6-478a00db90ce',\n",
       " 17: '4c7b258f-f2f3-4bf2-98c0-fa4e913a8aad',\n",
       " 18: 'ca5a4d7c-2f8d-4601-a7d1-120abc96e1a8',\n",
       " 19: 'a8fd551a-a9d3-4c9b-bea9-86e92d74e920',\n",
       " 20: '114ac29b-8831-49fb-9036-56105cbf0043',\n",
       " 21: 'aaa9fc05-8f7b-42a5-b237-70cc76768d86',\n",
       " 22: '77ce8db4-3d18-46d4-b26f-eb8d610721ce',\n",
       " 23: 'a5559e89-8be1-4a9a-b4c0-1dc54eec17a5',\n",
       " 24: 'd928715c-6377-46a4-a941-21e80de5e3b3',\n",
       " 25: '0df45919-8e97-4cf7-98f3-e7cc7b4c85c0',\n",
       " 26: '78a58e50-7fd6-4570-a58e-07e9cbfd4430',\n",
       " 27: '1d7bd2a3-5622-4a8a-bc28-bd354007bb0d',\n",
       " 28: '5f8be410-d5a0-4c88-891e-66502933988b',\n",
       " 29: '6de82003-849b-4129-98d0-f91a56fd46db',\n",
       " 30: '533a6ca9-9bda-4b63-848c-9441c27a21e4',\n",
       " 31: '36b6686a-e7d1-4ee8-9f03-c82049ab90c8',\n",
       " 32: '770e748b-9574-4f39-a4d9-2d9028f64ea5',\n",
       " 33: '600e6bf1-0e43-4d2e-9c11-fbf459b3f4ef',\n",
       " 34: '85adeec9-01f6-4b7a-9c77-16688810c16b',\n",
       " 35: '12390fdf-3d96-4ff5-b4c8-5ec3e26c6a30',\n",
       " 36: '37d2657e-ccee-4f8e-846a-cb3f7181c6f8',\n",
       " 37: '035a6e56-8fe8-4b50-ae80-bf7a946de528',\n",
       " 38: 'f8f6c030-7f9a-4436-a9f8-236a77a16dc2',\n",
       " 39: 'e8d17b15-4aa2-4531-a51d-7f99e4c577e8',\n",
       " 40: '1efbb79a-f3a7-41f1-b534-351d7e675406',\n",
       " 41: '961ddc10-8d7b-43c6-99a3-ea6a0bfa7850',\n",
       " 42: '78c90347-9cc7-4c4b-8117-d8a33ee67016',\n",
       " 43: '43c23233-355e-45e7-82fe-1434e68771c8',\n",
       " 44: '1747f1b0-245b-4587-99fa-da1cbb7989fc',\n",
       " 45: '0ae33319-9cf2-4ca3-a52c-2112503496a0',\n",
       " 46: '44025d61-88bb-486e-a717-09b865b8ed35',\n",
       " 47: '83dd83a2-8359-4adf-a74f-fe3409a3eb25',\n",
       " 48: 'f5be0e03-eaf7-4644-bde4-34e857e929de',\n",
       " 49: 'cd4dfc2c-3186-4588-92c1-0eaf589a13fc',\n",
       " 50: 'f38336ab-b129-49a7-8542-5d0e2df34828',\n",
       " 51: 'b23486f3-681c-49da-a2f0-4be14cd0b7ec',\n",
       " 52: 'd9036061-1db0-4d26-85a4-336c784d35bb',\n",
       " 53: '5a011294-d34a-4a36-9ae4-fc73bd2ef7cb',\n",
       " 54: '7b1fbe26-06d3-40f1-a079-3e24bcb4256f',\n",
       " 55: '55d223a6-64f0-4fec-9c28-3bf7d22d5f3f',\n",
       " 56: 'ee2c22d9-8e93-4919-ae31-40493bd477e2',\n",
       " 57: 'dc99c0d4-58ce-4b46-bc88-d53621118b06',\n",
       " 58: '12d3f020-8f47-4b9f-a7a4-1c6495c9970c',\n",
       " 59: '2110f3c5-24ec-456f-a8cd-ccb9c2b3cf47',\n",
       " 60: 'd01f0b44-bd26-4bd9-9bde-133199a9fc73',\n",
       " 61: UUID('bc42a220-8564-416c-ba99-65f75694cf95'),\n",
       " 62: UUID('0a881c8a-9a92-47e6-9a67-13b9c9be1ba2')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_faiss_vs_hf_v1.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete only works by specifying id\n",
    "\n",
    "loaded_faiss_vs_hf_v1.delete(ids=[result_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs_hf_v1.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'label': 'Response 1', 'source': 'current_conversation.json', 'file_name': 'current_conversation.json'}, page_content='{\"John\": \"testing out\", \"Roydon\": \"testing testing\"}')]\n"
     ]
    }
   ],
   "source": [
    "# Query directly\n",
    "results = loaded_faiss_vs_hf_v1.similarity_search(\n",
    "    \"\",\n",
    "    k=1,\n",
    "    filter={\"source\": \"current_conversation.json\"},\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load in environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs_hf_v3 = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v3\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "loaded_faiss_vs_hf_v3_new = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v3_new\", embeddings=embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the original index: 61\n",
      "Number of vectors in the new index: 62\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vectors in the original index:\", loaded_faiss_vs_hf_v3.index.ntotal)\n",
    "print(\"Number of vectors in the new index:\", loaded_faiss_vs_hf_v3_new.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_faiss_vs_hf_v3_new.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8d8e4089-f6d4-4659-af44-6ba4484674ef\n"
     ]
    }
   ],
   "source": [
    "result_id = loaded_faiss_vs_hf_v3_new.index_to_docstore_id[loaded_faiss_vs_hf_v3_new.index.ntotal-1]\n",
    "print(result_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete only works by specifying id\n",
    "loaded_faiss_vs_hf_v3_new.delete(ids=[result_id])\n",
    "loaded_faiss_vs_hf_v3_new.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\hugging_face\\\\faiss_vs_hf_v3_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load in environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ['HUGGING_FACE_ACCESS_TOKEN'],\n",
    "    model_name='BAAI/bge-base-en-v1.5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss_vs_hf_Sin_Yee = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\faiss_vectorstore_SinYee_v2\", embeddings=embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9ed90913-1141-435b-b58b-08a687feae9d\n"
     ]
    }
   ],
   "source": [
    "result_id = loaded_faiss_vs_hf_Sin_Yee.index_to_docstore_id[loaded_faiss_vs_hf_Sin_Yee.index.ntotal-1]\n",
    "print(result_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete only works by specifying id\n",
    "loaded_faiss_vs_hf_Sin_Yee.delete(ids=[result_id])\n",
    "loaded_faiss_vs_hf_Sin_Yee.save_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\MuteApp\\\\assets\\\\mockdata\\\\Sin Yee\\\\faiss_vectorstore_SinYee_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
